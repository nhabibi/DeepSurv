# Vanilla DeepSurv - Code Summary

## âœ… Verification Complete

This codebase is a **faithful vanilla implementation** of DeepSurv (Katzman et al., 2018) in PyTorch.

---

## ğŸ“Š Code Statistics

| File | Lines | Purpose |
|------|-------|---------|
| `config.py` | 44 | Vanilla hyperparameters matching paper |
| `model.py` | 82 | DeepSurv neural network (PyTorch) |
| `loss.py` | 122 | Cox PH loss (Efron/Breslow) |
| `data_loader.py` | 175 | Data loading & synthetic generation |
| `train.py` | 218 | Training with SGD + Nesterov momentum |
| `evaluation.py` | 157 | C-index & visualization |
| `main.py` | 191 | Main training script |
| **Total** | **~1000** | Clean, minimal, readable |

---

## âœ… Vanilla Configuration Confirmed

### Model Architecture
- âœ… Hidden layers: **[25, 25]** (original paper)
- âœ… Activation: **ReLU** (rectify)
- âœ… Dropout: **0.0** (disabled, as in original)
- âœ… Batch Normalization: **False** (disabled, as in original)
- âœ… Output: Single node (log hazard ratio)

### Training Parameters
- âœ… Optimizer: **SGD with Nesterov momentum** (original)
- âœ… Learning Rate: **1e-4** (with power decay 0.001)
- âœ… Momentum: **0.9**
- âœ… L2 Regularization: **10.0** (strong, as in original)
- âœ… L1 Regularization: **0.0**
- âœ… Batch Size: **64**
- âœ… Max Epochs: **2000**
- âœ… Early Stopping Patience: **2000**

### Loss Function
- âœ… Method: **Efron** approximation
- âœ… Type: Negative log partial likelihood (Cox PH)

---

## ğŸ¯ Key Differences from Original

| Aspect | Original (2016-2018) | This Implementation |
|--------|---------------------|---------------------|
| Framework | Theano + Lasagne | PyTorch |
| Python Version | Python 2/3 | Python 3.14+ |
| GPU Support | CUDA only | CUDA + MPS (Apple Silicon) |
| Dependencies | Legacy (Theano) | Modern (PyTorch) |

**Everything else matches the vanilla paper exactly!**

---

## ğŸš€ Usage

```bash
# Train on synthetic linear data (expected C-index: ~0.80)
python main.py --data-type linear --n-samples 5000 --n-features 10

# Train on synthetic Gaussian data (expected C-index: ~0.75)
python main.py --data-type gaussian --n-samples 5000 --n-features 10

# Train on your own data
python main.py --data path/to/data.csv

# Force CPU (disable GPU)
python main.py --cpu --data-type linear --n-samples 5000
```

---

## ğŸ“ Project Structure

```
DeepSurv/
â”œâ”€â”€ .git/              # Git repository
â”œâ”€â”€ .venv/             # Virtual environment
â”œâ”€â”€ .gitignore         # Git ignore file
â”œâ”€â”€ README.md          # Documentation
â”œâ”€â”€ requirements.txt   # Dependencies
â”œâ”€â”€ config.py          # Vanilla hyperparameters
â”œâ”€â”€ model.py           # DeepSurv network
â”œâ”€â”€ loss.py            # Cox PH loss
â”œâ”€â”€ data_loader.py     # Data utilities
â”œâ”€â”€ train.py           # Training loop
â”œâ”€â”€ evaluation.py      # C-index & plots
â””â”€â”€ main.py            # Entry point
```

**Output directories** (created at runtime):
- `models/` - Saved model checkpoints
- `results/` - Training curves, plots, metrics

---

## âœ… Phase 1 Status

- [x] Vanilla architecture implemented
- [x] Vanilla hyperparameters configured
- [x] SGD with Nesterov momentum
- [x] No Batch Normalization
- [x] No Dropout
- [x] Strong L2 regularization (10.0)
- [x] Power learning rate decay
- [x] Apple Silicon (MPS) support
- [ ] **Validation on synthetic data** (next step)
- [ ] Achieve C-index ~0.80 on linear data
- [ ] Achieve C-index ~0.75 on Gaussian data

---

## ğŸ“ Next Steps (Phase 1)

1. **Run experiments** on synthetic data
2. **Verify C-index** matches paper (Â±0.03)
3. **Test on METABRIC** dataset (C-index ~0.65-0.68)
4. **Document results** for Phase 1 completion

Once Phase 1 is complete, proceed to:
- **Phase 2**: Apply to SEER with comorbidity features
- **Phase 3**: Novel architectures and improvements

---

*Last updated: October 14, 2025*
*Codebase verified: Vanilla DeepSurv implementation*
